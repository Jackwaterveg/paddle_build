{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8340060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Paddle/paddle_build/notebook\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ae1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "wenetspeech_conformer_model='http://mobvoi-speech-public.ufile.ucloud.cn/public/wenet/wenetspeech/20211025_conformer_exp.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c67241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-18 03:15:28--  http://mobvoi-speech-public.ufile.ucloud.cn/public/wenet/wenetspeech/20211025_conformer_exp.tar.gz\n",
      "Connecting to 172.19.56.199:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 478183697 (456M) [application/x-gzip]\n",
      "Saving to: '20211025_conformer_exp.tar.gz'\n",
      "\n",
      "20211025_conformer_ 100%[===================>] 456.03M  3.52MB/s    in 2m 2s   \n",
      "\n",
      "2021-11-18 03:17:56 (3.73 MB/s) - '20211025_conformer_exp.tar.gz' saved [478183697/478183697]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget $wenetspeech_conformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59562017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211025_conformer_exp/\n",
      "20211025_conformer_exp/global_cmvn\n",
      "20211025_conformer_exp/words.txt\n",
      "20211025_conformer_exp/final.pt\n",
      "20211025_conformer_exp/train.yaml\n"
     ]
    }
   ],
   "source": [
    "!tar zxvf '20211025_conformer_exp.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b3f0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import paddle\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "225bac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_path='20211025_conformer_exp/train.yaml'\n",
    "model_path='20211025_conformer_exp/final.pt'\n",
    "cmvn_path='20211025_conformer_exp/global_cmvn'\n",
    "vocab_path='20211025_conformer_exp/words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff412f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5537 20211025_conformer_exp/words.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l $vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebf9383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accum_grad': 16,\n",
      " 'cmvn_file': 'exp/conformer/global_cmvn',\n",
      " 'dataset_conf': {'batch_conf': {'batch_size': 32, 'batch_type': 'static'},\n",
      "                  'fbank_conf': {'dither': 1.0,\n",
      "                                 'frame_length': 25,\n",
      "                                 'frame_shift': 10,\n",
      "                                 'num_mel_bins': 80},\n",
      "                  'filter_conf': {'max_length': 1200,\n",
      "                                  'min_length': 10,\n",
      "                                  'token_max_length': 100,\n",
      "                                  'token_min_length': 1},\n",
      "                  'resample_conf': {'resample_rate': 16000},\n",
      "                  'shuffle': True,\n",
      "                  'shuffle_conf': {'shuffle_size': 1500},\n",
      "                  'sort': True,\n",
      "                  'sort_conf': {'sort_size': 1000},\n",
      "                  'spec_aug': True,\n",
      "                  'spec_aug_conf': {'max_f': 30,\n",
      "                                    'max_t': 50,\n",
      "                                    'num_f_mask': 2,\n",
      "                                    'num_t_mask': 2},\n",
      "                  'speed_perturb': False},\n",
      " 'decoder': 'transformer',\n",
      " 'decoder_conf': {'attention_heads': 8,\n",
      "                  'dropout_rate': 0.1,\n",
      "                  'linear_units': 2048,\n",
      "                  'num_blocks': 6,\n",
      "                  'positional_dropout_rate': 0.1,\n",
      "                  'self_attention_dropout_rate': 0.0,\n",
      "                  'src_attention_dropout_rate': 0.0},\n",
      " 'encoder': 'conformer',\n",
      " 'encoder_conf': {'activation_type': 'swish',\n",
      "                  'attention_dropout_rate': 0.0,\n",
      "                  'attention_heads': 8,\n",
      "                  'cnn_module_kernel': 15,\n",
      "                  'cnn_module_norm': 'layer_norm',\n",
      "                  'dropout_rate': 0.1,\n",
      "                  'input_layer': 'conv2d',\n",
      "                  'linear_units': 2048,\n",
      "                  'normalize_before': True,\n",
      "                  'num_blocks': 12,\n",
      "                  'output_size': 512,\n",
      "                  'pos_enc_layer_type': 'rel_pos',\n",
      "                  'positional_dropout_rate': 0.1,\n",
      "                  'selfattention_layer_type': 'rel_selfattn',\n",
      "                  'use_cnn_module': True},\n",
      " 'grad_clip': 5,\n",
      " 'input_dim': 80,\n",
      " 'is_json_cmvn': True,\n",
      " 'log_interval': 100,\n",
      " 'max_epoch': 50,\n",
      " 'model_conf': {'ctc_weight': 0.3,\n",
      "                'length_normalized_loss': False,\n",
      "                'lsm_weight': 0.1},\n",
      " 'optim': 'adam',\n",
      " 'optim_conf': {'lr': 0.001},\n",
      " 'output_dim': 5537,\n",
      " 'scheduler': 'warmuplr',\n",
      " 'scheduler_conf': {'warmup_steps': 5000}}\n"
     ]
    }
   ],
   "source": [
    "conf = yaml.load(open(conf_path), yaml.FullLoader)\n",
    "pprint(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf56e99",
   "metadata": {},
   "source": [
    "# load model & dump to paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85aaddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1aaf8cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_state_dict = torch.load(model_path, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b101933f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.global_cmvn.mean \t torch.Size([80]) \t torch.float32\n",
      "encoder.global_cmvn.istd \t torch.Size([80]) \t torch.float32\n",
      "encoder.embed.conv.0.weight \t torch.Size([512, 1, 3, 3]) \t torch.float32\n",
      "encoder.embed.conv.0.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.embed.conv.2.weight \t torch.Size([512, 512, 3, 3]) \t torch.float32\n",
      "encoder.embed.conv.2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.embed.out.0.weight \t torch.Size([512, 9728]) \t torch.float32\n",
      "encoder.embed.out.0.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.after_norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.after_norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.0.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.0.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.0.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.0.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.0.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.0.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.0.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.0.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.0.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.0.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.0.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.0.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.0.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.1.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.1.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.1.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.1.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.1.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.1.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.1.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.1.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.1.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.1.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.1.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.1.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.1.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.2.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.2.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.2.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.2.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.2.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.2.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.2.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.2.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.2.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.2.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.2.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.2.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.2.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.3.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.3.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.3.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.3.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.3.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.3.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.3.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.3.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.3.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.3.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.3.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.3.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.3.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.4.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.4.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.4.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.4.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.4.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.4.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.4.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.4.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.4.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.4.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.4.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.4.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.4.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.5.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.5.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.5.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.5.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.5.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.5.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.5.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.5.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.5.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.5.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.5.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.5.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.5.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.6.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.6.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.6.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.6.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.6.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.6.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.6.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.6.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.6.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.6.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.6.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.6.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.6.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.7.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.7.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.7.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.7.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.7.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.7.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.7.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.7.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.7.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.7.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.7.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.7.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.7.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.8.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.8.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.8.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.8.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.8.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.8.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.8.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.8.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.8.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.8.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.8.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.8.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.8.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.9.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.9.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.9.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.9.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.9.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.9.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.9.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.9.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.9.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.9.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.9.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.9.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.9.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.10.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.10.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.10.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.10.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.10.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.10.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.10.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.10.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.10.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n",
      "encoder.encoders.10.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.10.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.10.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.10.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.pos_bias_u \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.pos_bias_v \t torch.Size([8, 64]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.self_attn.linear_pos.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "encoder.encoders.11.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.11.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.11.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.11.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.feed_forward_macaron.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "encoder.encoders.11.feed_forward_macaron.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "encoder.encoders.11.feed_forward_macaron.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "encoder.encoders.11.feed_forward_macaron.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.conv_module.pointwise_conv1.weight \t torch.Size([1024, 512, 1]) \t torch.float32\n",
      "encoder.encoders.11.conv_module.pointwise_conv1.bias \t torch.Size([1024]) \t torch.float32\n",
      "encoder.encoders.11.conv_module.depthwise_conv.weight \t torch.Size([512, 1, 15]) \t torch.float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.encoders.11.conv_module.depthwise_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.conv_module.norm.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.conv_module.norm.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.conv_module.pointwise_conv2.weight \t torch.Size([512, 512, 1]) \t torch.float32\n",
      "encoder.encoders.11.conv_module.pointwise_conv2.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.norm_ff.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.norm_ff.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.norm_mha.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.norm_mha.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.norm_ff_macaron.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.norm_ff_macaron.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.norm_conv.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.norm_conv.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.norm_final.weight \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.norm_final.bias \t torch.Size([512]) \t torch.float32\n",
      "encoder.encoders.11.concat_linear.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "encoder.encoders.11.concat_linear.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.embed.0.weight \t torch.Size([5537, 512]) \t torch.float32\n",
      "decoder.after_norm.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.after_norm.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.output_layer.weight \t torch.Size([5537, 512]) \t torch.float32\n",
      "decoder.output_layer.bias \t torch.Size([5537]) \t torch.float32\n",
      "decoder.decoders.0.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.0.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.0.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.0.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.0.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.src_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.0.src_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.src_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.0.src_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.src_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.0.src_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.src_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.0.src_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "decoder.decoders.0.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "decoder.decoders.0.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "decoder.decoders.0.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.norm1.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.norm1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.norm2.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.norm2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.norm3.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.norm3.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.concat_linear1.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.0.concat_linear1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.0.concat_linear2.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.0.concat_linear2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.1.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.1.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.1.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.1.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.src_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.1.src_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.src_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.1.src_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.src_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.1.src_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.src_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.1.src_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "decoder.decoders.1.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "decoder.decoders.1.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "decoder.decoders.1.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.norm1.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.norm1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.norm2.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.norm2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.norm3.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.norm3.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.concat_linear1.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.1.concat_linear1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.1.concat_linear2.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.1.concat_linear2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.2.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.2.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.2.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.2.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.src_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.2.src_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.src_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.2.src_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.src_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.2.src_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.src_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.2.src_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "decoder.decoders.2.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "decoder.decoders.2.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "decoder.decoders.2.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.norm1.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.norm1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.norm2.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.norm2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.norm3.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.norm3.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.concat_linear1.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.2.concat_linear1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.2.concat_linear2.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.2.concat_linear2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.3.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.3.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.3.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.3.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.src_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.3.src_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.src_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.3.src_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.src_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.3.src_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.src_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.3.src_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "decoder.decoders.3.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "decoder.decoders.3.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "decoder.decoders.3.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.norm1.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.norm1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.norm2.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.norm2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.norm3.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.norm3.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.concat_linear1.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.3.concat_linear1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.3.concat_linear2.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.3.concat_linear2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.4.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.4.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.4.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.4.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.src_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.4.src_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.src_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.4.src_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.src_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.4.src_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.src_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.4.src_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "decoder.decoders.4.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "decoder.decoders.4.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "decoder.decoders.4.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.norm1.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.norm1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.norm2.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.norm2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.norm3.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.norm3.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.concat_linear1.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.4.concat_linear1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.4.concat_linear2.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.4.concat_linear2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.self_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.5.self_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.self_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.5.self_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.self_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.5.self_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.self_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.5.self_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.src_attn.linear_q.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.5.src_attn.linear_q.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.src_attn.linear_k.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.5.src_attn.linear_k.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.src_attn.linear_v.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.5.src_attn.linear_v.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.src_attn.linear_out.weight \t torch.Size([512, 512]) \t torch.float32\n",
      "decoder.decoders.5.src_attn.linear_out.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.feed_forward.w_1.weight \t torch.Size([2048, 512]) \t torch.float32\n",
      "decoder.decoders.5.feed_forward.w_1.bias \t torch.Size([2048]) \t torch.float32\n",
      "decoder.decoders.5.feed_forward.w_2.weight \t torch.Size([512, 2048]) \t torch.float32\n",
      "decoder.decoders.5.feed_forward.w_2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.norm1.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.norm1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.norm2.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.norm2.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.norm3.weight \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.norm3.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.concat_linear1.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.5.concat_linear1.bias \t torch.Size([512]) \t torch.float32\n",
      "decoder.decoders.5.concat_linear2.weight \t torch.Size([512, 1024]) \t torch.float32\n",
      "decoder.decoders.5.concat_linear2.bias \t torch.Size([512]) \t torch.float32\n",
      "ctc.ctc_lo.weight \t torch.Size([5537, 512]) \t torch.float32\n",
      "ctc.ctc_lo.bias \t torch.Size([5537]) \t torch.float32\n"
     ]
    }
   ],
   "source": [
    "param_cnt=0\n",
    "for key, val in model_state_dict.items():\n",
    "    print(key, \"\\t\", val.shape, \"\\t\", val.dtype)\n",
    "    param_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9f2a5704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665\n"
     ]
    }
   ],
   "source": [
    "print(param_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "918165f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump torch model encoder to paddle encoder\n",
    "def torch_paddle_param(model_state_dict):\n",
    "    paddle_state_dict = {}\n",
    "    for n, p in model_state_dict.items():\n",
    "        print(f'-> name: {n} dim: {p.ndim}')\n",
    "        \n",
    "        # change norm.mean and norm variance\n",
    "        name_change=True\n",
    "        if 'norm.running_mean' in n:\n",
    "            new_n = n.replace('norm.running_', 'norm._')\n",
    "        elif 'norm.running_var' in n:\n",
    "            new_n = n.replace('norm.running_var', 'norm._variance')\n",
    "        else:\n",
    "            name_change=False\n",
    "            new_n = n\n",
    "        if name_change:\n",
    "            print(f\"\\t norm mean/var: {n} -> {new_n}\")\n",
    "\n",
    "        p = p.cpu().detach().numpy()\n",
    "        \n",
    "        # weight which is rank 2, transpose it\n",
    "        if n.endswith('weight') and p.ndim == 2:\n",
    "            new_p = p.T\n",
    "            print(f\"\\t liner transpose: {n}: {p.shape} -> {new_p.shape}\")\n",
    "        else:\n",
    "            new_p = p\n",
    "        \n",
    "        # text embedding layer\n",
    "        if 'decoder.embed.0.weight' in n:\n",
    "            new_p = p\n",
    "\n",
    "        if 'global_cmvn.mean' in n:\n",
    "            print(\"\\t cmvn:\", p, p.dtype)\n",
    "        if 'global_cmvn.istd' in n:\n",
    "            print(\"\\t istd:\", p, p.dtype)\n",
    "\n",
    "        paddle_state_dict[new_n] = new_p\n",
    "    return paddle_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4c03a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> name: encoder.global_cmvn.mean dim: 1\n",
      "\t cmvn: [11.837256 12.473205 13.416769 14.07741  14.692714 15.134645 15.425054\n",
      " 15.520304 15.66498  15.682886 15.831343 15.901057 16.043106 16.141926\n",
      " 16.14606  16.172684 16.13231  16.065538 16.170683 15.998217 15.867838\n",
      " 16.081026 15.909136 16.032068 15.948578 16.035398 15.919972 16.01213\n",
      " 15.935732 15.914797 15.949415 15.914241 15.920595 15.979177 15.986891\n",
      " 16.046032 16.110855 16.11682  16.129877 16.085758 16.134706 16.098186\n",
      " 16.202894 16.195677 16.265987 16.368603 16.485243 16.530725 16.586134\n",
      " 16.682056 16.643587 16.623293 16.638262 16.703993 16.758455 16.818436\n",
      " 16.887299 16.890385 16.816685 16.731005 16.674946 16.562819 16.506945\n",
      " 16.427153 16.336958 16.224352 16.122593 16.074572 16.045864 15.997705\n",
      " 15.955503 15.925531 15.884871 15.847951 15.812487 15.79125  15.698866\n",
      " 15.451056 15.043111 14.453489] float32\n",
      "-> name: encoder.global_cmvn.istd dim: 1\n",
      "\t istd: [0.33298537 0.3044571  0.29546332 0.30167487 0.30146667 0.2972203\n",
      " 0.29171112 0.28384522 0.28523776 0.29010338 0.2948906  0.29547352\n",
      " 0.29620674 0.29563197 0.29470083 0.29632273 0.29766065 0.3003492\n",
      " 0.30211455 0.30174902 0.30016822 0.3033776  0.30356762 0.30603644\n",
      " 0.30671984 0.30692285 0.30770084 0.3067656  0.30552036 0.30690625\n",
      " 0.3072341  0.3088911  0.30912516 0.30986765 0.31000063 0.3096373\n",
      " 0.30939674 0.3091856  0.3096874  0.30854377 0.30930805 0.30873135\n",
      " 0.3081481  0.30722052 0.30732656 0.3064066  0.30390662 0.30213097\n",
      " 0.3014575  0.30144924 0.30039048 0.29975197 0.29932147 0.29809597\n",
      " 0.29504582 0.29251    0.29285434 0.29285946 0.29226425 0.29342884\n",
      " 0.29299378 0.2921846  0.29174167 0.28991738 0.2888153  0.2870271\n",
      " 0.28432825 0.28270337 0.28035107 0.2782083  0.27589947 0.27325842\n",
      " 0.27104923 0.26880762 0.26814315 0.2699856  0.2693469  0.2673888\n",
      " 0.2683234  0.27021357] float32\n",
      "-> name: encoder.embed.conv.0.weight dim: 4\n",
      "-> name: encoder.embed.conv.0.bias dim: 1\n",
      "-> name: encoder.embed.conv.2.weight dim: 4\n",
      "-> name: encoder.embed.conv.2.bias dim: 1\n",
      "-> name: encoder.embed.out.0.weight dim: 2\n",
      "\t liner transpose: encoder.embed.out.0.weight: (512, 9728) -> (9728, 512)\n",
      "-> name: encoder.embed.out.0.bias dim: 1\n",
      "-> name: encoder.after_norm.weight dim: 1\n",
      "-> name: encoder.after_norm.bias dim: 1\n",
      "-> name: encoder.encoders.0.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.0.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.0.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.0.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.0.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.0.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.0.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.0.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.0.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.0.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.0.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.0.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.0.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.0.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.0.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.0.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.0.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.0.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.0.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.0.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.0.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.0.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.0.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.0.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.0.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.0.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.0.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.0.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.0.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.0.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.0.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.0.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.0.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.0.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.0.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.0.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.0.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.0.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.0.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.0.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.0.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.0.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.0.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.0.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.0.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.0.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.0.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.0.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.0.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.1.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.1.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.1.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.1.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.1.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.1.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.1.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.1.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.1.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.1.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.1.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.1.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.1.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.1.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.1.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.1.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.1.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.1.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.1.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.1.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.1.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.1.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.1.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.1.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.1.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.1.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.1.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.1.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.1.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.1.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.1.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.1.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.1.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.1.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.1.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.1.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.1.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.1.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.1.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.1.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.1.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.1.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.1.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.1.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.1.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.1.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.1.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.1.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.1.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.2.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.2.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.2.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.2.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.2.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.2.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.2.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.2.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.2.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.2.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.2.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.2.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.2.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.2.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.2.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.2.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.2.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.2.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.2.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.2.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.2.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.2.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.2.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.2.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.2.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.2.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.2.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.2.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.2.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.2.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.2.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.2.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.2.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.2.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.2.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.2.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.2.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.2.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.2.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.2.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.2.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.2.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.2.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.2.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.2.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.2.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.2.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.2.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.2.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.3.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.3.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.3.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.3.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.3.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.3.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.3.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.3.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.3.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.3.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.3.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.3.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.3.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.3.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.3.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.3.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.3.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.3.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.3.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.3.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.3.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.3.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.3.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.3.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.3.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.3.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.3.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.3.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.3.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.3.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.3.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.3.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.3.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.3.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.3.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.3.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.3.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.3.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.3.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.3.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.3.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.3.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.3.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.3.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.3.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.3.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.3.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.3.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.3.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.4.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.4.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.4.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.4.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.4.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.4.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.4.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.4.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.4.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.4.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.4.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.4.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.4.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.4.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.4.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.4.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.4.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.4.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.4.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.4.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.4.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.4.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.4.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.4.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.4.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.4.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.4.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.4.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.4.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.4.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.4.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.4.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.4.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.4.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.4.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.4.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.4.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.4.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.4.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.4.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.4.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.4.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.4.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.4.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.4.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.4.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.4.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.4.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.4.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.5.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.5.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.5.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.5.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.5.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.5.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.5.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.5.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.5.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.5.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.5.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.5.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.5.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.5.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.5.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.5.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.5.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.5.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.5.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.5.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.5.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.5.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.5.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.5.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.5.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.5.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.5.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.5.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.5.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.5.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.5.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.5.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.5.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.5.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.5.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.5.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.5.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.5.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.5.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.5.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.5.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.5.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.5.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.5.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.5.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.5.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.5.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.5.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.5.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.6.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.6.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.6.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.6.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.6.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.6.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.6.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.6.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.6.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.6.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.6.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.6.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.6.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.6.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.6.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.6.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.6.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.6.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.6.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.6.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.6.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.6.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.6.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.6.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.6.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.6.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.6.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.6.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.6.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.6.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.6.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.6.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.6.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.6.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.6.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.6.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.6.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.6.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.6.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.6.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.6.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.6.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.6.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.6.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.6.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.6.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.6.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.6.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.6.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.7.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.7.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.7.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.7.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.7.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.7.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.7.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.7.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.7.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.7.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.7.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.7.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.7.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.7.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.7.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.7.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.7.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.7.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.7.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.7.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.7.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.7.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.7.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.7.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.7.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.7.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.7.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.7.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.7.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.7.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.7.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.7.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.7.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.7.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.7.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.7.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.7.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.7.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.7.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.7.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.7.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.7.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.7.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.7.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.7.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.7.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.7.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.7.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.7.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.8.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.8.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.8.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.8.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.8.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.8.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.8.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.8.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.8.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.8.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.8.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.8.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.8.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.8.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.8.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.8.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.8.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.8.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.8.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.8.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.8.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.8.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.8.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.8.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.8.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.8.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.8.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.8.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.8.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.8.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.8.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.8.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.8.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.8.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.8.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.8.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.8.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.8.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.8.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.8.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.8.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.8.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.8.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.8.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.8.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.8.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.8.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.8.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.8.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.9.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.9.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.9.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.9.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.9.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.9.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.9.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.9.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.9.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.9.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.9.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.9.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.9.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.9.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.9.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.9.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.9.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.9.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.9.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.9.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.9.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.9.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.9.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.9.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.9.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.9.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.9.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.9.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.9.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.9.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.9.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.9.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.9.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.9.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.9.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.9.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.9.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.9.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.9.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.9.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.9.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.9.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.9.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.9.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.9.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.9.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.9.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.9.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.9.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.10.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.10.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.10.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.10.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.10.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.10.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.10.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.10.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.10.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.10.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.10.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.10.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.10.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.10.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.10.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.10.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.10.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.10.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.10.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.10.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.10.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.10.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.10.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.10.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.10.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.10.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.10.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.10.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.10.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.10.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.10.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.10.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.10.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.10.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.10.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.10.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.10.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.10.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.10.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.10.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.10.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.10.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.10.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.10.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.10.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.10.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.10.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.10.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.10.concat_linear.bias dim: 1\n",
      "-> name: encoder.encoders.11.self_attn.pos_bias_u dim: 2\n",
      "-> name: encoder.encoders.11.self_attn.pos_bias_v dim: 2\n",
      "-> name: encoder.encoders.11.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.11.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.11.self_attn.linear_q.bias dim: 1\n",
      "-> name: encoder.encoders.11.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.11.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.11.self_attn.linear_k.bias dim: 1\n",
      "-> name: encoder.encoders.11.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.11.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.11.self_attn.linear_v.bias dim: 1\n",
      "-> name: encoder.encoders.11.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.11.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.11.self_attn.linear_out.bias dim: 1\n",
      "-> name: encoder.encoders.11.self_attn.linear_pos.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.11.self_attn.linear_pos.weight: (512, 512) -> (512, 512)\n",
      "-> name: encoder.encoders.11.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.11.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.11.feed_forward.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.11.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.11.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.11.feed_forward.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.11.feed_forward_macaron.w_1.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.11.feed_forward_macaron.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: encoder.encoders.11.feed_forward_macaron.w_1.bias dim: 1\n",
      "-> name: encoder.encoders.11.feed_forward_macaron.w_2.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.11.feed_forward_macaron.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: encoder.encoders.11.feed_forward_macaron.w_2.bias dim: 1\n",
      "-> name: encoder.encoders.11.conv_module.pointwise_conv1.weight dim: 3\n",
      "-> name: encoder.encoders.11.conv_module.pointwise_conv1.bias dim: 1\n",
      "-> name: encoder.encoders.11.conv_module.depthwise_conv.weight dim: 3\n",
      "-> name: encoder.encoders.11.conv_module.depthwise_conv.bias dim: 1\n",
      "-> name: encoder.encoders.11.conv_module.norm.weight dim: 1\n",
      "-> name: encoder.encoders.11.conv_module.norm.bias dim: 1\n",
      "-> name: encoder.encoders.11.conv_module.pointwise_conv2.weight dim: 3\n",
      "-> name: encoder.encoders.11.conv_module.pointwise_conv2.bias dim: 1\n",
      "-> name: encoder.encoders.11.norm_ff.weight dim: 1\n",
      "-> name: encoder.encoders.11.norm_ff.bias dim: 1\n",
      "-> name: encoder.encoders.11.norm_mha.weight dim: 1\n",
      "-> name: encoder.encoders.11.norm_mha.bias dim: 1\n",
      "-> name: encoder.encoders.11.norm_ff_macaron.weight dim: 1\n",
      "-> name: encoder.encoders.11.norm_ff_macaron.bias dim: 1\n",
      "-> name: encoder.encoders.11.norm_conv.weight dim: 1\n",
      "-> name: encoder.encoders.11.norm_conv.bias dim: 1\n",
      "-> name: encoder.encoders.11.norm_final.weight dim: 1\n",
      "-> name: encoder.encoders.11.norm_final.bias dim: 1\n",
      "-> name: encoder.encoders.11.concat_linear.weight dim: 2\n",
      "\t liner transpose: encoder.encoders.11.concat_linear.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: encoder.encoders.11.concat_linear.bias dim: 1\n",
      "-> name: decoder.embed.0.weight dim: 2\n",
      "\t liner transpose: decoder.embed.0.weight: (5537, 512) -> (512, 5537)\n",
      "-> name: decoder.after_norm.weight dim: 1\n",
      "-> name: decoder.after_norm.bias dim: 1\n",
      "-> name: decoder.output_layer.weight dim: 2\n",
      "\t liner transpose: decoder.output_layer.weight: (5537, 512) -> (512, 5537)\n",
      "-> name: decoder.output_layer.bias dim: 1\n",
      "-> name: decoder.decoders.0.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.0.self_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.0.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.0.self_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.0.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.0.self_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.0.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.0.self_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.0.src_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.src_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.0.src_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.0.src_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.src_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.0.src_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.0.src_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.src_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.0.src_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.0.src_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.src_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.0.src_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.0.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: decoder.decoders.0.feed_forward.w_1.bias dim: 1\n",
      "-> name: decoder.decoders.0.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: decoder.decoders.0.feed_forward.w_2.bias dim: 1\n",
      "-> name: decoder.decoders.0.norm1.weight dim: 1\n",
      "-> name: decoder.decoders.0.norm1.bias dim: 1\n",
      "-> name: decoder.decoders.0.norm2.weight dim: 1\n",
      "-> name: decoder.decoders.0.norm2.bias dim: 1\n",
      "-> name: decoder.decoders.0.norm3.weight dim: 1\n",
      "-> name: decoder.decoders.0.norm3.bias dim: 1\n",
      "-> name: decoder.decoders.0.concat_linear1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.concat_linear1.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.0.concat_linear1.bias dim: 1\n",
      "-> name: decoder.decoders.0.concat_linear2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.0.concat_linear2.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.0.concat_linear2.bias dim: 1\n",
      "-> name: decoder.decoders.1.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.1.self_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.1.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.1.self_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.1.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.1.self_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.1.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.1.self_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.1.src_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.src_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.1.src_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.1.src_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.src_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.1.src_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.1.src_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.src_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.1.src_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.1.src_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.src_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.1.src_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.1.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: decoder.decoders.1.feed_forward.w_1.bias dim: 1\n",
      "-> name: decoder.decoders.1.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: decoder.decoders.1.feed_forward.w_2.bias dim: 1\n",
      "-> name: decoder.decoders.1.norm1.weight dim: 1\n",
      "-> name: decoder.decoders.1.norm1.bias dim: 1\n",
      "-> name: decoder.decoders.1.norm2.weight dim: 1\n",
      "-> name: decoder.decoders.1.norm2.bias dim: 1\n",
      "-> name: decoder.decoders.1.norm3.weight dim: 1\n",
      "-> name: decoder.decoders.1.norm3.bias dim: 1\n",
      "-> name: decoder.decoders.1.concat_linear1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.concat_linear1.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.1.concat_linear1.bias dim: 1\n",
      "-> name: decoder.decoders.1.concat_linear2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.1.concat_linear2.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.1.concat_linear2.bias dim: 1\n",
      "-> name: decoder.decoders.2.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.2.self_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.2.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.2.self_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.2.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.2.self_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.2.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.2.self_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.2.src_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.src_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.2.src_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.2.src_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.src_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.2.src_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.2.src_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.src_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.2.src_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.2.src_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.src_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.2.src_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.2.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: decoder.decoders.2.feed_forward.w_1.bias dim: 1\n",
      "-> name: decoder.decoders.2.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: decoder.decoders.2.feed_forward.w_2.bias dim: 1\n",
      "-> name: decoder.decoders.2.norm1.weight dim: 1\n",
      "-> name: decoder.decoders.2.norm1.bias dim: 1\n",
      "-> name: decoder.decoders.2.norm2.weight dim: 1\n",
      "-> name: decoder.decoders.2.norm2.bias dim: 1\n",
      "-> name: decoder.decoders.2.norm3.weight dim: 1\n",
      "-> name: decoder.decoders.2.norm3.bias dim: 1\n",
      "-> name: decoder.decoders.2.concat_linear1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.concat_linear1.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.2.concat_linear1.bias dim: 1\n",
      "-> name: decoder.decoders.2.concat_linear2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.2.concat_linear2.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.2.concat_linear2.bias dim: 1\n",
      "-> name: decoder.decoders.3.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.3.self_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.3.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.3.self_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.3.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.3.self_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.3.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.3.self_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.3.src_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.src_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.3.src_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.3.src_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.src_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.3.src_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.3.src_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.src_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.3.src_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.3.src_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.src_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.3.src_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.3.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: decoder.decoders.3.feed_forward.w_1.bias dim: 1\n",
      "-> name: decoder.decoders.3.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: decoder.decoders.3.feed_forward.w_2.bias dim: 1\n",
      "-> name: decoder.decoders.3.norm1.weight dim: 1\n",
      "-> name: decoder.decoders.3.norm1.bias dim: 1\n",
      "-> name: decoder.decoders.3.norm2.weight dim: 1\n",
      "-> name: decoder.decoders.3.norm2.bias dim: 1\n",
      "-> name: decoder.decoders.3.norm3.weight dim: 1\n",
      "-> name: decoder.decoders.3.norm3.bias dim: 1\n",
      "-> name: decoder.decoders.3.concat_linear1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.concat_linear1.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.3.concat_linear1.bias dim: 1\n",
      "-> name: decoder.decoders.3.concat_linear2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.3.concat_linear2.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.3.concat_linear2.bias dim: 1\n",
      "-> name: decoder.decoders.4.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.4.self_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.4.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.4.self_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.4.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.4.self_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.4.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.4.self_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.4.src_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.src_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.4.src_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.4.src_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.src_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.4.src_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.4.src_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.src_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.4.src_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.4.src_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.src_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.4.src_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.4.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: decoder.decoders.4.feed_forward.w_1.bias dim: 1\n",
      "-> name: decoder.decoders.4.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: decoder.decoders.4.feed_forward.w_2.bias dim: 1\n",
      "-> name: decoder.decoders.4.norm1.weight dim: 1\n",
      "-> name: decoder.decoders.4.norm1.bias dim: 1\n",
      "-> name: decoder.decoders.4.norm2.weight dim: 1\n",
      "-> name: decoder.decoders.4.norm2.bias dim: 1\n",
      "-> name: decoder.decoders.4.norm3.weight dim: 1\n",
      "-> name: decoder.decoders.4.norm3.bias dim: 1\n",
      "-> name: decoder.decoders.4.concat_linear1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.concat_linear1.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.4.concat_linear1.bias dim: 1\n",
      "-> name: decoder.decoders.4.concat_linear2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.4.concat_linear2.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.4.concat_linear2.bias dim: 1\n",
      "-> name: decoder.decoders.5.self_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.self_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.5.self_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.5.self_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.self_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.5.self_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.5.self_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.self_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.5.self_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.5.self_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.self_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.5.self_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.5.src_attn.linear_q.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.src_attn.linear_q.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.5.src_attn.linear_q.bias dim: 1\n",
      "-> name: decoder.decoders.5.src_attn.linear_k.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.src_attn.linear_k.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.5.src_attn.linear_k.bias dim: 1\n",
      "-> name: decoder.decoders.5.src_attn.linear_v.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.src_attn.linear_v.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.5.src_attn.linear_v.bias dim: 1\n",
      "-> name: decoder.decoders.5.src_attn.linear_out.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.src_attn.linear_out.weight: (512, 512) -> (512, 512)\n",
      "-> name: decoder.decoders.5.src_attn.linear_out.bias dim: 1\n",
      "-> name: decoder.decoders.5.feed_forward.w_1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.feed_forward.w_1.weight: (2048, 512) -> (512, 2048)\n",
      "-> name: decoder.decoders.5.feed_forward.w_1.bias dim: 1\n",
      "-> name: decoder.decoders.5.feed_forward.w_2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.feed_forward.w_2.weight: (512, 2048) -> (2048, 512)\n",
      "-> name: decoder.decoders.5.feed_forward.w_2.bias dim: 1\n",
      "-> name: decoder.decoders.5.norm1.weight dim: 1\n",
      "-> name: decoder.decoders.5.norm1.bias dim: 1\n",
      "-> name: decoder.decoders.5.norm2.weight dim: 1\n",
      "-> name: decoder.decoders.5.norm2.bias dim: 1\n",
      "-> name: decoder.decoders.5.norm3.weight dim: 1\n",
      "-> name: decoder.decoders.5.norm3.bias dim: 1\n",
      "-> name: decoder.decoders.5.concat_linear1.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.concat_linear1.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.5.concat_linear1.bias dim: 1\n",
      "-> name: decoder.decoders.5.concat_linear2.weight dim: 2\n",
      "\t liner transpose: decoder.decoders.5.concat_linear2.weight: (512, 1024) -> (1024, 512)\n",
      "-> name: decoder.decoders.5.concat_linear2.bias dim: 1\n",
      "-> name: ctc.ctc_lo.weight dim: 2\n",
      "\t liner transpose: ctc.ctc_lo.weight: (5537, 512) -> (512, 5537)\n",
      "-> name: ctc.ctc_lo.bias dim: 1\n"
     ]
    }
   ],
   "source": [
    "paddle_model_state_dict = torch_paddle_param(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9847a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c949ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all param equal: True\n"
     ]
    }
   ],
   "source": [
    "eq = []\n",
    "for key, val in model_state_dict.items():\n",
    "    if 'decoder.embed.0.weight' in key:\n",
    "        # text embeding\n",
    "        val = val\n",
    "    elif key.endswith('weight') and val.ndim == 2:\n",
    "        # linear weight\n",
    "        val = val.T  \n",
    "    eq.append(np.allclose(val, paddle_model_state_dict[key]))         \n",
    "print(\"all param equal:\", np.array(eq).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4ed0ec",
   "metadata": {},
   "source": [
    "## save paddle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26925da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/DeepSpeech-2.x/tools/venv/lib/python3.7/site-packages/paddle/framework/io.py:415: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  if isinstance(obj, collections.Iterable) and not isinstance(obj, (\n"
     ]
    }
   ],
   "source": [
    "paddle.save(paddle_model_state_dict, \"wenetspeech.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0f85951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4641e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"wenetspeech.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5cbbdc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8bf27bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encoder.global_cmvn.mean', 'encoder.global_cmvn.istd', 'encoder.embed.conv.0.weight', 'encoder.embed.conv.0.bias', 'encoder.embed.conv.2.weight', 'encoder.embed.conv.2.bias', 'encoder.embed.out.0.weight', 'encoder.embed.out.0.bias', 'encoder.after_norm.weight', 'encoder.after_norm.bias', 'encoder.encoders.0.self_attn.pos_bias_u', 'encoder.encoders.0.self_attn.pos_bias_v', 'encoder.encoders.0.self_attn.linear_q.weight', 'encoder.encoders.0.self_attn.linear_q.bias', 'encoder.encoders.0.self_attn.linear_k.weight', 'encoder.encoders.0.self_attn.linear_k.bias', 'encoder.encoders.0.self_attn.linear_v.weight', 'encoder.encoders.0.self_attn.linear_v.bias', 'encoder.encoders.0.self_attn.linear_out.weight', 'encoder.encoders.0.self_attn.linear_out.bias', 'encoder.encoders.0.self_attn.linear_pos.weight', 'encoder.encoders.0.feed_forward.w_1.weight', 'encoder.encoders.0.feed_forward.w_1.bias', 'encoder.encoders.0.feed_forward.w_2.weight', 'encoder.encoders.0.feed_forward.w_2.bias', 'encoder.encoders.0.feed_forward_macaron.w_1.weight', 'encoder.encoders.0.feed_forward_macaron.w_1.bias', 'encoder.encoders.0.feed_forward_macaron.w_2.weight', 'encoder.encoders.0.feed_forward_macaron.w_2.bias', 'encoder.encoders.0.conv_module.pointwise_conv1.weight', 'encoder.encoders.0.conv_module.pointwise_conv1.bias', 'encoder.encoders.0.conv_module.depthwise_conv.weight', 'encoder.encoders.0.conv_module.depthwise_conv.bias', 'encoder.encoders.0.conv_module.norm.weight', 'encoder.encoders.0.conv_module.norm.bias', 'encoder.encoders.0.conv_module.pointwise_conv2.weight', 'encoder.encoders.0.conv_module.pointwise_conv2.bias', 'encoder.encoders.0.norm_ff.weight', 'encoder.encoders.0.norm_ff.bias', 'encoder.encoders.0.norm_mha.weight', 'encoder.encoders.0.norm_mha.bias', 'encoder.encoders.0.norm_ff_macaron.weight', 'encoder.encoders.0.norm_ff_macaron.bias', 'encoder.encoders.0.norm_conv.weight', 'encoder.encoders.0.norm_conv.bias', 'encoder.encoders.0.norm_final.weight', 'encoder.encoders.0.norm_final.bias', 'encoder.encoders.0.concat_linear.weight', 'encoder.encoders.0.concat_linear.bias', 'encoder.encoders.1.self_attn.pos_bias_u', 'encoder.encoders.1.self_attn.pos_bias_v', 'encoder.encoders.1.self_attn.linear_q.weight', 'encoder.encoders.1.self_attn.linear_q.bias', 'encoder.encoders.1.self_attn.linear_k.weight', 'encoder.encoders.1.self_attn.linear_k.bias', 'encoder.encoders.1.self_attn.linear_v.weight', 'encoder.encoders.1.self_attn.linear_v.bias', 'encoder.encoders.1.self_attn.linear_out.weight', 'encoder.encoders.1.self_attn.linear_out.bias', 'encoder.encoders.1.self_attn.linear_pos.weight', 'encoder.encoders.1.feed_forward.w_1.weight', 'encoder.encoders.1.feed_forward.w_1.bias', 'encoder.encoders.1.feed_forward.w_2.weight', 'encoder.encoders.1.feed_forward.w_2.bias', 'encoder.encoders.1.feed_forward_macaron.w_1.weight', 'encoder.encoders.1.feed_forward_macaron.w_1.bias', 'encoder.encoders.1.feed_forward_macaron.w_2.weight', 'encoder.encoders.1.feed_forward_macaron.w_2.bias', 'encoder.encoders.1.conv_module.pointwise_conv1.weight', 'encoder.encoders.1.conv_module.pointwise_conv1.bias', 'encoder.encoders.1.conv_module.depthwise_conv.weight', 'encoder.encoders.1.conv_module.depthwise_conv.bias', 'encoder.encoders.1.conv_module.norm.weight', 'encoder.encoders.1.conv_module.norm.bias', 'encoder.encoders.1.conv_module.pointwise_conv2.weight', 'encoder.encoders.1.conv_module.pointwise_conv2.bias', 'encoder.encoders.1.norm_ff.weight', 'encoder.encoders.1.norm_ff.bias', 'encoder.encoders.1.norm_mha.weight', 'encoder.encoders.1.norm_mha.bias', 'encoder.encoders.1.norm_ff_macaron.weight', 'encoder.encoders.1.norm_ff_macaron.bias', 'encoder.encoders.1.norm_conv.weight', 'encoder.encoders.1.norm_conv.bias', 'encoder.encoders.1.norm_final.weight', 'encoder.encoders.1.norm_final.bias', 'encoder.encoders.1.concat_linear.weight', 'encoder.encoders.1.concat_linear.bias', 'encoder.encoders.2.self_attn.pos_bias_u', 'encoder.encoders.2.self_attn.pos_bias_v', 'encoder.encoders.2.self_attn.linear_q.weight', 'encoder.encoders.2.self_attn.linear_q.bias', 'encoder.encoders.2.self_attn.linear_k.weight', 'encoder.encoders.2.self_attn.linear_k.bias', 'encoder.encoders.2.self_attn.linear_v.weight', 'encoder.encoders.2.self_attn.linear_v.bias', 'encoder.encoders.2.self_attn.linear_out.weight', 'encoder.encoders.2.self_attn.linear_out.bias', 'encoder.encoders.2.self_attn.linear_pos.weight', 'encoder.encoders.2.feed_forward.w_1.weight', 'encoder.encoders.2.feed_forward.w_1.bias', 'encoder.encoders.2.feed_forward.w_2.weight', 'encoder.encoders.2.feed_forward.w_2.bias', 'encoder.encoders.2.feed_forward_macaron.w_1.weight', 'encoder.encoders.2.feed_forward_macaron.w_1.bias', 'encoder.encoders.2.feed_forward_macaron.w_2.weight', 'encoder.encoders.2.feed_forward_macaron.w_2.bias', 'encoder.encoders.2.conv_module.pointwise_conv1.weight', 'encoder.encoders.2.conv_module.pointwise_conv1.bias', 'encoder.encoders.2.conv_module.depthwise_conv.weight', 'encoder.encoders.2.conv_module.depthwise_conv.bias', 'encoder.encoders.2.conv_module.norm.weight', 'encoder.encoders.2.conv_module.norm.bias', 'encoder.encoders.2.conv_module.pointwise_conv2.weight', 'encoder.encoders.2.conv_module.pointwise_conv2.bias', 'encoder.encoders.2.norm_ff.weight', 'encoder.encoders.2.norm_ff.bias', 'encoder.encoders.2.norm_mha.weight', 'encoder.encoders.2.norm_mha.bias', 'encoder.encoders.2.norm_ff_macaron.weight', 'encoder.encoders.2.norm_ff_macaron.bias', 'encoder.encoders.2.norm_conv.weight', 'encoder.encoders.2.norm_conv.bias', 'encoder.encoders.2.norm_final.weight', 'encoder.encoders.2.norm_final.bias', 'encoder.encoders.2.concat_linear.weight', 'encoder.encoders.2.concat_linear.bias', 'encoder.encoders.3.self_attn.pos_bias_u', 'encoder.encoders.3.self_attn.pos_bias_v', 'encoder.encoders.3.self_attn.linear_q.weight', 'encoder.encoders.3.self_attn.linear_q.bias', 'encoder.encoders.3.self_attn.linear_k.weight', 'encoder.encoders.3.self_attn.linear_k.bias', 'encoder.encoders.3.self_attn.linear_v.weight', 'encoder.encoders.3.self_attn.linear_v.bias', 'encoder.encoders.3.self_attn.linear_out.weight', 'encoder.encoders.3.self_attn.linear_out.bias', 'encoder.encoders.3.self_attn.linear_pos.weight', 'encoder.encoders.3.feed_forward.w_1.weight', 'encoder.encoders.3.feed_forward.w_1.bias', 'encoder.encoders.3.feed_forward.w_2.weight', 'encoder.encoders.3.feed_forward.w_2.bias', 'encoder.encoders.3.feed_forward_macaron.w_1.weight', 'encoder.encoders.3.feed_forward_macaron.w_1.bias', 'encoder.encoders.3.feed_forward_macaron.w_2.weight', 'encoder.encoders.3.feed_forward_macaron.w_2.bias', 'encoder.encoders.3.conv_module.pointwise_conv1.weight', 'encoder.encoders.3.conv_module.pointwise_conv1.bias', 'encoder.encoders.3.conv_module.depthwise_conv.weight', 'encoder.encoders.3.conv_module.depthwise_conv.bias', 'encoder.encoders.3.conv_module.norm.weight', 'encoder.encoders.3.conv_module.norm.bias', 'encoder.encoders.3.conv_module.pointwise_conv2.weight', 'encoder.encoders.3.conv_module.pointwise_conv2.bias', 'encoder.encoders.3.norm_ff.weight', 'encoder.encoders.3.norm_ff.bias', 'encoder.encoders.3.norm_mha.weight', 'encoder.encoders.3.norm_mha.bias', 'encoder.encoders.3.norm_ff_macaron.weight', 'encoder.encoders.3.norm_ff_macaron.bias', 'encoder.encoders.3.norm_conv.weight', 'encoder.encoders.3.norm_conv.bias', 'encoder.encoders.3.norm_final.weight', 'encoder.encoders.3.norm_final.bias', 'encoder.encoders.3.concat_linear.weight', 'encoder.encoders.3.concat_linear.bias', 'encoder.encoders.4.self_attn.pos_bias_u', 'encoder.encoders.4.self_attn.pos_bias_v', 'encoder.encoders.4.self_attn.linear_q.weight', 'encoder.encoders.4.self_attn.linear_q.bias', 'encoder.encoders.4.self_attn.linear_k.weight', 'encoder.encoders.4.self_attn.linear_k.bias', 'encoder.encoders.4.self_attn.linear_v.weight', 'encoder.encoders.4.self_attn.linear_v.bias', 'encoder.encoders.4.self_attn.linear_out.weight', 'encoder.encoders.4.self_attn.linear_out.bias', 'encoder.encoders.4.self_attn.linear_pos.weight', 'encoder.encoders.4.feed_forward.w_1.weight', 'encoder.encoders.4.feed_forward.w_1.bias', 'encoder.encoders.4.feed_forward.w_2.weight', 'encoder.encoders.4.feed_forward.w_2.bias', 'encoder.encoders.4.feed_forward_macaron.w_1.weight', 'encoder.encoders.4.feed_forward_macaron.w_1.bias', 'encoder.encoders.4.feed_forward_macaron.w_2.weight', 'encoder.encoders.4.feed_forward_macaron.w_2.bias', 'encoder.encoders.4.conv_module.pointwise_conv1.weight', 'encoder.encoders.4.conv_module.pointwise_conv1.bias', 'encoder.encoders.4.conv_module.depthwise_conv.weight', 'encoder.encoders.4.conv_module.depthwise_conv.bias', 'encoder.encoders.4.conv_module.norm.weight', 'encoder.encoders.4.conv_module.norm.bias', 'encoder.encoders.4.conv_module.pointwise_conv2.weight', 'encoder.encoders.4.conv_module.pointwise_conv2.bias', 'encoder.encoders.4.norm_ff.weight', 'encoder.encoders.4.norm_ff.bias', 'encoder.encoders.4.norm_mha.weight', 'encoder.encoders.4.norm_mha.bias', 'encoder.encoders.4.norm_ff_macaron.weight', 'encoder.encoders.4.norm_ff_macaron.bias', 'encoder.encoders.4.norm_conv.weight', 'encoder.encoders.4.norm_conv.bias', 'encoder.encoders.4.norm_final.weight', 'encoder.encoders.4.norm_final.bias', 'encoder.encoders.4.concat_linear.weight', 'encoder.encoders.4.concat_linear.bias', 'encoder.encoders.5.self_attn.pos_bias_u', 'encoder.encoders.5.self_attn.pos_bias_v', 'encoder.encoders.5.self_attn.linear_q.weight', 'encoder.encoders.5.self_attn.linear_q.bias', 'encoder.encoders.5.self_attn.linear_k.weight', 'encoder.encoders.5.self_attn.linear_k.bias', 'encoder.encoders.5.self_attn.linear_v.weight', 'encoder.encoders.5.self_attn.linear_v.bias', 'encoder.encoders.5.self_attn.linear_out.weight', 'encoder.encoders.5.self_attn.linear_out.bias', 'encoder.encoders.5.self_attn.linear_pos.weight', 'encoder.encoders.5.feed_forward.w_1.weight', 'encoder.encoders.5.feed_forward.w_1.bias', 'encoder.encoders.5.feed_forward.w_2.weight', 'encoder.encoders.5.feed_forward.w_2.bias', 'encoder.encoders.5.feed_forward_macaron.w_1.weight', 'encoder.encoders.5.feed_forward_macaron.w_1.bias', 'encoder.encoders.5.feed_forward_macaron.w_2.weight', 'encoder.encoders.5.feed_forward_macaron.w_2.bias', 'encoder.encoders.5.conv_module.pointwise_conv1.weight', 'encoder.encoders.5.conv_module.pointwise_conv1.bias', 'encoder.encoders.5.conv_module.depthwise_conv.weight', 'encoder.encoders.5.conv_module.depthwise_conv.bias', 'encoder.encoders.5.conv_module.norm.weight', 'encoder.encoders.5.conv_module.norm.bias', 'encoder.encoders.5.conv_module.pointwise_conv2.weight', 'encoder.encoders.5.conv_module.pointwise_conv2.bias', 'encoder.encoders.5.norm_ff.weight', 'encoder.encoders.5.norm_ff.bias', 'encoder.encoders.5.norm_mha.weight', 'encoder.encoders.5.norm_mha.bias', 'encoder.encoders.5.norm_ff_macaron.weight', 'encoder.encoders.5.norm_ff_macaron.bias', 'encoder.encoders.5.norm_conv.weight', 'encoder.encoders.5.norm_conv.bias', 'encoder.encoders.5.norm_final.weight', 'encoder.encoders.5.norm_final.bias', 'encoder.encoders.5.concat_linear.weight', 'encoder.encoders.5.concat_linear.bias', 'encoder.encoders.6.self_attn.pos_bias_u', 'encoder.encoders.6.self_attn.pos_bias_v', 'encoder.encoders.6.self_attn.linear_q.weight', 'encoder.encoders.6.self_attn.linear_q.bias', 'encoder.encoders.6.self_attn.linear_k.weight', 'encoder.encoders.6.self_attn.linear_k.bias', 'encoder.encoders.6.self_attn.linear_v.weight', 'encoder.encoders.6.self_attn.linear_v.bias', 'encoder.encoders.6.self_attn.linear_out.weight', 'encoder.encoders.6.self_attn.linear_out.bias', 'encoder.encoders.6.self_attn.linear_pos.weight', 'encoder.encoders.6.feed_forward.w_1.weight', 'encoder.encoders.6.feed_forward.w_1.bias', 'encoder.encoders.6.feed_forward.w_2.weight', 'encoder.encoders.6.feed_forward.w_2.bias', 'encoder.encoders.6.feed_forward_macaron.w_1.weight', 'encoder.encoders.6.feed_forward_macaron.w_1.bias', 'encoder.encoders.6.feed_forward_macaron.w_2.weight', 'encoder.encoders.6.feed_forward_macaron.w_2.bias', 'encoder.encoders.6.conv_module.pointwise_conv1.weight', 'encoder.encoders.6.conv_module.pointwise_conv1.bias', 'encoder.encoders.6.conv_module.depthwise_conv.weight', 'encoder.encoders.6.conv_module.depthwise_conv.bias', 'encoder.encoders.6.conv_module.norm.weight', 'encoder.encoders.6.conv_module.norm.bias', 'encoder.encoders.6.conv_module.pointwise_conv2.weight', 'encoder.encoders.6.conv_module.pointwise_conv2.bias', 'encoder.encoders.6.norm_ff.weight', 'encoder.encoders.6.norm_ff.bias', 'encoder.encoders.6.norm_mha.weight', 'encoder.encoders.6.norm_mha.bias', 'encoder.encoders.6.norm_ff_macaron.weight', 'encoder.encoders.6.norm_ff_macaron.bias', 'encoder.encoders.6.norm_conv.weight', 'encoder.encoders.6.norm_conv.bias', 'encoder.encoders.6.norm_final.weight', 'encoder.encoders.6.norm_final.bias', 'encoder.encoders.6.concat_linear.weight', 'encoder.encoders.6.concat_linear.bias', 'encoder.encoders.7.self_attn.pos_bias_u', 'encoder.encoders.7.self_attn.pos_bias_v', 'encoder.encoders.7.self_attn.linear_q.weight', 'encoder.encoders.7.self_attn.linear_q.bias', 'encoder.encoders.7.self_attn.linear_k.weight', 'encoder.encoders.7.self_attn.linear_k.bias', 'encoder.encoders.7.self_attn.linear_v.weight', 'encoder.encoders.7.self_attn.linear_v.bias', 'encoder.encoders.7.self_attn.linear_out.weight', 'encoder.encoders.7.self_attn.linear_out.bias', 'encoder.encoders.7.self_attn.linear_pos.weight', 'encoder.encoders.7.feed_forward.w_1.weight', 'encoder.encoders.7.feed_forward.w_1.bias', 'encoder.encoders.7.feed_forward.w_2.weight', 'encoder.encoders.7.feed_forward.w_2.bias', 'encoder.encoders.7.feed_forward_macaron.w_1.weight', 'encoder.encoders.7.feed_forward_macaron.w_1.bias', 'encoder.encoders.7.feed_forward_macaron.w_2.weight', 'encoder.encoders.7.feed_forward_macaron.w_2.bias', 'encoder.encoders.7.conv_module.pointwise_conv1.weight', 'encoder.encoders.7.conv_module.pointwise_conv1.bias', 'encoder.encoders.7.conv_module.depthwise_conv.weight', 'encoder.encoders.7.conv_module.depthwise_conv.bias', 'encoder.encoders.7.conv_module.norm.weight', 'encoder.encoders.7.conv_module.norm.bias', 'encoder.encoders.7.conv_module.pointwise_conv2.weight', 'encoder.encoders.7.conv_module.pointwise_conv2.bias', 'encoder.encoders.7.norm_ff.weight', 'encoder.encoders.7.norm_ff.bias', 'encoder.encoders.7.norm_mha.weight', 'encoder.encoders.7.norm_mha.bias', 'encoder.encoders.7.norm_ff_macaron.weight', 'encoder.encoders.7.norm_ff_macaron.bias', 'encoder.encoders.7.norm_conv.weight', 'encoder.encoders.7.norm_conv.bias', 'encoder.encoders.7.norm_final.weight', 'encoder.encoders.7.norm_final.bias', 'encoder.encoders.7.concat_linear.weight', 'encoder.encoders.7.concat_linear.bias', 'encoder.encoders.8.self_attn.pos_bias_u', 'encoder.encoders.8.self_attn.pos_bias_v', 'encoder.encoders.8.self_attn.linear_q.weight', 'encoder.encoders.8.self_attn.linear_q.bias', 'encoder.encoders.8.self_attn.linear_k.weight', 'encoder.encoders.8.self_attn.linear_k.bias', 'encoder.encoders.8.self_attn.linear_v.weight', 'encoder.encoders.8.self_attn.linear_v.bias', 'encoder.encoders.8.self_attn.linear_out.weight', 'encoder.encoders.8.self_attn.linear_out.bias', 'encoder.encoders.8.self_attn.linear_pos.weight', 'encoder.encoders.8.feed_forward.w_1.weight', 'encoder.encoders.8.feed_forward.w_1.bias', 'encoder.encoders.8.feed_forward.w_2.weight', 'encoder.encoders.8.feed_forward.w_2.bias', 'encoder.encoders.8.feed_forward_macaron.w_1.weight', 'encoder.encoders.8.feed_forward_macaron.w_1.bias', 'encoder.encoders.8.feed_forward_macaron.w_2.weight', 'encoder.encoders.8.feed_forward_macaron.w_2.bias', 'encoder.encoders.8.conv_module.pointwise_conv1.weight', 'encoder.encoders.8.conv_module.pointwise_conv1.bias', 'encoder.encoders.8.conv_module.depthwise_conv.weight', 'encoder.encoders.8.conv_module.depthwise_conv.bias', 'encoder.encoders.8.conv_module.norm.weight', 'encoder.encoders.8.conv_module.norm.bias', 'encoder.encoders.8.conv_module.pointwise_conv2.weight', 'encoder.encoders.8.conv_module.pointwise_conv2.bias', 'encoder.encoders.8.norm_ff.weight', 'encoder.encoders.8.norm_ff.bias', 'encoder.encoders.8.norm_mha.weight', 'encoder.encoders.8.norm_mha.bias', 'encoder.encoders.8.norm_ff_macaron.weight', 'encoder.encoders.8.norm_ff_macaron.bias', 'encoder.encoders.8.norm_conv.weight', 'encoder.encoders.8.norm_conv.bias', 'encoder.encoders.8.norm_final.weight', 'encoder.encoders.8.norm_final.bias', 'encoder.encoders.8.concat_linear.weight', 'encoder.encoders.8.concat_linear.bias', 'encoder.encoders.9.self_attn.pos_bias_u', 'encoder.encoders.9.self_attn.pos_bias_v', 'encoder.encoders.9.self_attn.linear_q.weight', 'encoder.encoders.9.self_attn.linear_q.bias', 'encoder.encoders.9.self_attn.linear_k.weight', 'encoder.encoders.9.self_attn.linear_k.bias', 'encoder.encoders.9.self_attn.linear_v.weight', 'encoder.encoders.9.self_attn.linear_v.bias', 'encoder.encoders.9.self_attn.linear_out.weight', 'encoder.encoders.9.self_attn.linear_out.bias', 'encoder.encoders.9.self_attn.linear_pos.weight', 'encoder.encoders.9.feed_forward.w_1.weight', 'encoder.encoders.9.feed_forward.w_1.bias', 'encoder.encoders.9.feed_forward.w_2.weight', 'encoder.encoders.9.feed_forward.w_2.bias', 'encoder.encoders.9.feed_forward_macaron.w_1.weight', 'encoder.encoders.9.feed_forward_macaron.w_1.bias', 'encoder.encoders.9.feed_forward_macaron.w_2.weight', 'encoder.encoders.9.feed_forward_macaron.w_2.bias', 'encoder.encoders.9.conv_module.pointwise_conv1.weight', 'encoder.encoders.9.conv_module.pointwise_conv1.bias', 'encoder.encoders.9.conv_module.depthwise_conv.weight', 'encoder.encoders.9.conv_module.depthwise_conv.bias', 'encoder.encoders.9.conv_module.norm.weight', 'encoder.encoders.9.conv_module.norm.bias', 'encoder.encoders.9.conv_module.pointwise_conv2.weight', 'encoder.encoders.9.conv_module.pointwise_conv2.bias', 'encoder.encoders.9.norm_ff.weight', 'encoder.encoders.9.norm_ff.bias', 'encoder.encoders.9.norm_mha.weight', 'encoder.encoders.9.norm_mha.bias', 'encoder.encoders.9.norm_ff_macaron.weight', 'encoder.encoders.9.norm_ff_macaron.bias', 'encoder.encoders.9.norm_conv.weight', 'encoder.encoders.9.norm_conv.bias', 'encoder.encoders.9.norm_final.weight', 'encoder.encoders.9.norm_final.bias', 'encoder.encoders.9.concat_linear.weight', 'encoder.encoders.9.concat_linear.bias', 'encoder.encoders.10.self_attn.pos_bias_u', 'encoder.encoders.10.self_attn.pos_bias_v', 'encoder.encoders.10.self_attn.linear_q.weight', 'encoder.encoders.10.self_attn.linear_q.bias', 'encoder.encoders.10.self_attn.linear_k.weight', 'encoder.encoders.10.self_attn.linear_k.bias', 'encoder.encoders.10.self_attn.linear_v.weight', 'encoder.encoders.10.self_attn.linear_v.bias', 'encoder.encoders.10.self_attn.linear_out.weight', 'encoder.encoders.10.self_attn.linear_out.bias', 'encoder.encoders.10.self_attn.linear_pos.weight', 'encoder.encoders.10.feed_forward.w_1.weight', 'encoder.encoders.10.feed_forward.w_1.bias', 'encoder.encoders.10.feed_forward.w_2.weight', 'encoder.encoders.10.feed_forward.w_2.bias', 'encoder.encoders.10.feed_forward_macaron.w_1.weight', 'encoder.encoders.10.feed_forward_macaron.w_1.bias', 'encoder.encoders.10.feed_forward_macaron.w_2.weight', 'encoder.encoders.10.feed_forward_macaron.w_2.bias', 'encoder.encoders.10.conv_module.pointwise_conv1.weight', 'encoder.encoders.10.conv_module.pointwise_conv1.bias', 'encoder.encoders.10.conv_module.depthwise_conv.weight', 'encoder.encoders.10.conv_module.depthwise_conv.bias', 'encoder.encoders.10.conv_module.norm.weight', 'encoder.encoders.10.conv_module.norm.bias', 'encoder.encoders.10.conv_module.pointwise_conv2.weight', 'encoder.encoders.10.conv_module.pointwise_conv2.bias', 'encoder.encoders.10.norm_ff.weight', 'encoder.encoders.10.norm_ff.bias', 'encoder.encoders.10.norm_mha.weight', 'encoder.encoders.10.norm_mha.bias', 'encoder.encoders.10.norm_ff_macaron.weight', 'encoder.encoders.10.norm_ff_macaron.bias', 'encoder.encoders.10.norm_conv.weight', 'encoder.encoders.10.norm_conv.bias', 'encoder.encoders.10.norm_final.weight', 'encoder.encoders.10.norm_final.bias', 'encoder.encoders.10.concat_linear.weight', 'encoder.encoders.10.concat_linear.bias', 'encoder.encoders.11.self_attn.pos_bias_u', 'encoder.encoders.11.self_attn.pos_bias_v', 'encoder.encoders.11.self_attn.linear_q.weight', 'encoder.encoders.11.self_attn.linear_q.bias', 'encoder.encoders.11.self_attn.linear_k.weight', 'encoder.encoders.11.self_attn.linear_k.bias', 'encoder.encoders.11.self_attn.linear_v.weight', 'encoder.encoders.11.self_attn.linear_v.bias', 'encoder.encoders.11.self_attn.linear_out.weight', 'encoder.encoders.11.self_attn.linear_out.bias', 'encoder.encoders.11.self_attn.linear_pos.weight', 'encoder.encoders.11.feed_forward.w_1.weight', 'encoder.encoders.11.feed_forward.w_1.bias', 'encoder.encoders.11.feed_forward.w_2.weight', 'encoder.encoders.11.feed_forward.w_2.bias', 'encoder.encoders.11.feed_forward_macaron.w_1.weight', 'encoder.encoders.11.feed_forward_macaron.w_1.bias', 'encoder.encoders.11.feed_forward_macaron.w_2.weight', 'encoder.encoders.11.feed_forward_macaron.w_2.bias', 'encoder.encoders.11.conv_module.pointwise_conv1.weight', 'encoder.encoders.11.conv_module.pointwise_conv1.bias', 'encoder.encoders.11.conv_module.depthwise_conv.weight', 'encoder.encoders.11.conv_module.depthwise_conv.bias', 'encoder.encoders.11.conv_module.norm.weight', 'encoder.encoders.11.conv_module.norm.bias', 'encoder.encoders.11.conv_module.pointwise_conv2.weight', 'encoder.encoders.11.conv_module.pointwise_conv2.bias', 'encoder.encoders.11.norm_ff.weight', 'encoder.encoders.11.norm_ff.bias', 'encoder.encoders.11.norm_mha.weight', 'encoder.encoders.11.norm_mha.bias', 'encoder.encoders.11.norm_ff_macaron.weight', 'encoder.encoders.11.norm_ff_macaron.bias', 'encoder.encoders.11.norm_conv.weight', 'encoder.encoders.11.norm_conv.bias', 'encoder.encoders.11.norm_final.weight', 'encoder.encoders.11.norm_final.bias', 'encoder.encoders.11.concat_linear.weight', 'encoder.encoders.11.concat_linear.bias', 'decoder.embed.0.weight', 'decoder.after_norm.weight', 'decoder.after_norm.bias', 'decoder.output_layer.weight', 'decoder.output_layer.bias', 'decoder.decoders.0.self_attn.linear_q.weight', 'decoder.decoders.0.self_attn.linear_q.bias', 'decoder.decoders.0.self_attn.linear_k.weight', 'decoder.decoders.0.self_attn.linear_k.bias', 'decoder.decoders.0.self_attn.linear_v.weight', 'decoder.decoders.0.self_attn.linear_v.bias', 'decoder.decoders.0.self_attn.linear_out.weight', 'decoder.decoders.0.self_attn.linear_out.bias', 'decoder.decoders.0.src_attn.linear_q.weight', 'decoder.decoders.0.src_attn.linear_q.bias', 'decoder.decoders.0.src_attn.linear_k.weight', 'decoder.decoders.0.src_attn.linear_k.bias', 'decoder.decoders.0.src_attn.linear_v.weight', 'decoder.decoders.0.src_attn.linear_v.bias', 'decoder.decoders.0.src_attn.linear_out.weight', 'decoder.decoders.0.src_attn.linear_out.bias', 'decoder.decoders.0.feed_forward.w_1.weight', 'decoder.decoders.0.feed_forward.w_1.bias', 'decoder.decoders.0.feed_forward.w_2.weight', 'decoder.decoders.0.feed_forward.w_2.bias', 'decoder.decoders.0.norm1.weight', 'decoder.decoders.0.norm1.bias', 'decoder.decoders.0.norm2.weight', 'decoder.decoders.0.norm2.bias', 'decoder.decoders.0.norm3.weight', 'decoder.decoders.0.norm3.bias', 'decoder.decoders.0.concat_linear1.weight', 'decoder.decoders.0.concat_linear1.bias', 'decoder.decoders.0.concat_linear2.weight', 'decoder.decoders.0.concat_linear2.bias', 'decoder.decoders.1.self_attn.linear_q.weight', 'decoder.decoders.1.self_attn.linear_q.bias', 'decoder.decoders.1.self_attn.linear_k.weight', 'decoder.decoders.1.self_attn.linear_k.bias', 'decoder.decoders.1.self_attn.linear_v.weight', 'decoder.decoders.1.self_attn.linear_v.bias', 'decoder.decoders.1.self_attn.linear_out.weight', 'decoder.decoders.1.self_attn.linear_out.bias', 'decoder.decoders.1.src_attn.linear_q.weight', 'decoder.decoders.1.src_attn.linear_q.bias', 'decoder.decoders.1.src_attn.linear_k.weight', 'decoder.decoders.1.src_attn.linear_k.bias', 'decoder.decoders.1.src_attn.linear_v.weight', 'decoder.decoders.1.src_attn.linear_v.bias', 'decoder.decoders.1.src_attn.linear_out.weight', 'decoder.decoders.1.src_attn.linear_out.bias', 'decoder.decoders.1.feed_forward.w_1.weight', 'decoder.decoders.1.feed_forward.w_1.bias', 'decoder.decoders.1.feed_forward.w_2.weight', 'decoder.decoders.1.feed_forward.w_2.bias', 'decoder.decoders.1.norm1.weight', 'decoder.decoders.1.norm1.bias', 'decoder.decoders.1.norm2.weight', 'decoder.decoders.1.norm2.bias', 'decoder.decoders.1.norm3.weight', 'decoder.decoders.1.norm3.bias', 'decoder.decoders.1.concat_linear1.weight', 'decoder.decoders.1.concat_linear1.bias', 'decoder.decoders.1.concat_linear2.weight', 'decoder.decoders.1.concat_linear2.bias', 'decoder.decoders.2.self_attn.linear_q.weight', 'decoder.decoders.2.self_attn.linear_q.bias', 'decoder.decoders.2.self_attn.linear_k.weight', 'decoder.decoders.2.self_attn.linear_k.bias', 'decoder.decoders.2.self_attn.linear_v.weight', 'decoder.decoders.2.self_attn.linear_v.bias', 'decoder.decoders.2.self_attn.linear_out.weight', 'decoder.decoders.2.self_attn.linear_out.bias', 'decoder.decoders.2.src_attn.linear_q.weight', 'decoder.decoders.2.src_attn.linear_q.bias', 'decoder.decoders.2.src_attn.linear_k.weight', 'decoder.decoders.2.src_attn.linear_k.bias', 'decoder.decoders.2.src_attn.linear_v.weight', 'decoder.decoders.2.src_attn.linear_v.bias', 'decoder.decoders.2.src_attn.linear_out.weight', 'decoder.decoders.2.src_attn.linear_out.bias', 'decoder.decoders.2.feed_forward.w_1.weight', 'decoder.decoders.2.feed_forward.w_1.bias', 'decoder.decoders.2.feed_forward.w_2.weight', 'decoder.decoders.2.feed_forward.w_2.bias', 'decoder.decoders.2.norm1.weight', 'decoder.decoders.2.norm1.bias', 'decoder.decoders.2.norm2.weight', 'decoder.decoders.2.norm2.bias', 'decoder.decoders.2.norm3.weight', 'decoder.decoders.2.norm3.bias', 'decoder.decoders.2.concat_linear1.weight', 'decoder.decoders.2.concat_linear1.bias', 'decoder.decoders.2.concat_linear2.weight', 'decoder.decoders.2.concat_linear2.bias', 'decoder.decoders.3.self_attn.linear_q.weight', 'decoder.decoders.3.self_attn.linear_q.bias', 'decoder.decoders.3.self_attn.linear_k.weight', 'decoder.decoders.3.self_attn.linear_k.bias', 'decoder.decoders.3.self_attn.linear_v.weight', 'decoder.decoders.3.self_attn.linear_v.bias', 'decoder.decoders.3.self_attn.linear_out.weight', 'decoder.decoders.3.self_attn.linear_out.bias', 'decoder.decoders.3.src_attn.linear_q.weight', 'decoder.decoders.3.src_attn.linear_q.bias', 'decoder.decoders.3.src_attn.linear_k.weight', 'decoder.decoders.3.src_attn.linear_k.bias', 'decoder.decoders.3.src_attn.linear_v.weight', 'decoder.decoders.3.src_attn.linear_v.bias', 'decoder.decoders.3.src_attn.linear_out.weight', 'decoder.decoders.3.src_attn.linear_out.bias', 'decoder.decoders.3.feed_forward.w_1.weight', 'decoder.decoders.3.feed_forward.w_1.bias', 'decoder.decoders.3.feed_forward.w_2.weight', 'decoder.decoders.3.feed_forward.w_2.bias', 'decoder.decoders.3.norm1.weight', 'decoder.decoders.3.norm1.bias', 'decoder.decoders.3.norm2.weight', 'decoder.decoders.3.norm2.bias', 'decoder.decoders.3.norm3.weight', 'decoder.decoders.3.norm3.bias', 'decoder.decoders.3.concat_linear1.weight', 'decoder.decoders.3.concat_linear1.bias', 'decoder.decoders.3.concat_linear2.weight', 'decoder.decoders.3.concat_linear2.bias', 'decoder.decoders.4.self_attn.linear_q.weight', 'decoder.decoders.4.self_attn.linear_q.bias', 'decoder.decoders.4.self_attn.linear_k.weight', 'decoder.decoders.4.self_attn.linear_k.bias', 'decoder.decoders.4.self_attn.linear_v.weight', 'decoder.decoders.4.self_attn.linear_v.bias', 'decoder.decoders.4.self_attn.linear_out.weight', 'decoder.decoders.4.self_attn.linear_out.bias', 'decoder.decoders.4.src_attn.linear_q.weight', 'decoder.decoders.4.src_attn.linear_q.bias', 'decoder.decoders.4.src_attn.linear_k.weight', 'decoder.decoders.4.src_attn.linear_k.bias', 'decoder.decoders.4.src_attn.linear_v.weight', 'decoder.decoders.4.src_attn.linear_v.bias', 'decoder.decoders.4.src_attn.linear_out.weight', 'decoder.decoders.4.src_attn.linear_out.bias', 'decoder.decoders.4.feed_forward.w_1.weight', 'decoder.decoders.4.feed_forward.w_1.bias', 'decoder.decoders.4.feed_forward.w_2.weight', 'decoder.decoders.4.feed_forward.w_2.bias', 'decoder.decoders.4.norm1.weight', 'decoder.decoders.4.norm1.bias', 'decoder.decoders.4.norm2.weight', 'decoder.decoders.4.norm2.bias', 'decoder.decoders.4.norm3.weight', 'decoder.decoders.4.norm3.bias', 'decoder.decoders.4.concat_linear1.weight', 'decoder.decoders.4.concat_linear1.bias', 'decoder.decoders.4.concat_linear2.weight', 'decoder.decoders.4.concat_linear2.bias', 'decoder.decoders.5.self_attn.linear_q.weight', 'decoder.decoders.5.self_attn.linear_q.bias', 'decoder.decoders.5.self_attn.linear_k.weight', 'decoder.decoders.5.self_attn.linear_k.bias', 'decoder.decoders.5.self_attn.linear_v.weight', 'decoder.decoders.5.self_attn.linear_v.bias', 'decoder.decoders.5.self_attn.linear_out.weight', 'decoder.decoders.5.self_attn.linear_out.bias', 'decoder.decoders.5.src_attn.linear_q.weight', 'decoder.decoders.5.src_attn.linear_q.bias', 'decoder.decoders.5.src_attn.linear_k.weight', 'decoder.decoders.5.src_attn.linear_k.bias', 'decoder.decoders.5.src_attn.linear_v.weight', 'decoder.decoders.5.src_attn.linear_v.bias', 'decoder.decoders.5.src_attn.linear_out.weight', 'decoder.decoders.5.src_attn.linear_out.bias', 'decoder.decoders.5.feed_forward.w_1.weight', 'decoder.decoders.5.feed_forward.w_1.bias', 'decoder.decoders.5.feed_forward.w_2.weight', 'decoder.decoders.5.feed_forward.w_2.bias', 'decoder.decoders.5.norm1.weight', 'decoder.decoders.5.norm1.bias', 'decoder.decoders.5.norm2.weight', 'decoder.decoders.5.norm2.bias', 'decoder.decoders.5.norm3.weight', 'decoder.decoders.5.norm3.bias', 'decoder.decoders.5.concat_linear1.weight', 'decoder.decoders.5.concat_linear1.bias', 'decoder.decoders.5.concat_linear2.weight', 'decoder.decoders.5.concat_linear2.bias', 'ctc.ctc_lo.weight', 'ctc.ctc_lo.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec58f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
